{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4a4262b82d9e345",
   "metadata": {},
   "source": [
    "## Power Grid Analysis\n",
    "\n",
    "For our AIPI 510 Data Storytelling project, we intend to use the EIA.gov EIA-930 Data. This shows the generation and energy consumption for the electrical powergrid in the United States."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782376888c6c1cb6",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "We're going to load a full year's worth of data. This is looking at July 2024 - June 2025. We'll download both files now."
   ]
  },
  {
   "cell_type": "code",
   "id": "fab49266ef12342",
   "metadata": {},
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "path = Path()\n",
    "\n",
    "files = {\n",
    "    \"EIA930_BALANCE_2025_Jan_Jun.csv\": \"https://www.eia.gov/electricity/gridmonitor/sixMonthFiles/EIA930_BALANCE_2025_Jan_Jun.csv\",\n",
    "    \"EIA930_BALANCE_2024_Jul_Dec.csv\": \"https://www.eia.gov/electricity/gridmonitor/sixMonthFiles/EIA930_BALANCE_2024_Jul_Dec.csv\",\n",
    "    \"EIA930_Reference_Tables.xlsx\": \"https://www.eia.gov/electricity/930-content/EIA930_Reference_Tables.xlsx\"\n",
    "}\n",
    "\n",
    "# Download each file\n",
    "for key, value in files.items():\n",
    "    filename = path / key\n",
    "    url = value\n",
    "    # If the file does not already exist in the directory, download it\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc8d7134fdb1c240",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "Our first step is to understand the data and read the rows and columns that exist. We need to explore and understand what all of the columns actually mean before we can work with the data."
   ]
  },
  {
   "cell_type": "code",
   "id": "edddadc95de9a6c8",
   "metadata": {},
   "source": [
    "df1 = pd.read_csv(\"EIA930_BALANCE_2024_Jul_Dec.csv\")\n",
    "df2 = pd.read_csv(\"EIA930_BALANCE_2025_Jan_Jun.csv\")\n",
    "\n",
    "# Concat both dataframes\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "df[\"UTC Time at End of Hour\"] = pd.to_datetime(df[\"UTC Time at End of Hour\"])\n",
    "\n",
    "print(df[\"UTC Time at End of Hour\"].min(), \"to\", df[\"UTC Time at End of Hour\"].max())\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "df.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "663750c491c90b96",
   "metadata": {},
   "source": [
    "## Understanding the data\n",
    "\n",
    "(What we talked about me doing: Data generation what BAs mean, Biases, Explain Demand vs Generation, time series data and it is hourly, difference between Demand imputed and adjusted, explain interchange. Go through each energy source and put it on a list. Drop and Impute and explain why.)\n",
    "\n",
    "At first, there appear to be a lot of columns. If we look at the first ten, we can see that there are a lot of NaN values. However, the reason is because this is for only one balancing authority. Each balancing authority uses different types of power generation to meet demand. We're only seeing the first ten hours from the AECI balancing authority. It would be illogical to assume that one authority could have *every* type of power generation. If we look at another balancing authority, we can see the missing values are different. Below is the AVA balancing authority. You can see that they generate power with solar, whereas AECI does not. The key takeaway is that different balancing authorities have different ways of generating power."
   ]
  },
  {
   "cell_type": "code",
   "id": "463ac664fbbbeba2",
   "metadata": {},
   "source": "print(df.iloc[4419:4429])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6b05212caa359a81",
   "metadata": {},
   "source": [
    "## Explaining the meaning of our columns\n",
    "\n",
    "After a quick glance at the data, we can tell based on the dates and times that we have time series data down to the hour. However, there are a couple things that are not as intuitive that are explained below:\n",
    "\n",
    "First, it is important to understand what some of the terminology means:\n",
    "\n",
    " - Balancing Authority (BA's): are the companies responsible for balancing electricity supply, demand, and interchange on their electric systems in real time. There are many BAs and the spreadsheet with more information about what they mean can be found here: https://view.officeapps.live.com/op/view.aspx?src=https%3A%2F%2Fwww.eia.gov%2Felectricity%2F930-content%2FEIA930_Reference_Tables.xlsx&wdOrigin=BROWSELINK\n",
    "\n",
    " - Demand: Derived by taking the total metered net electricity generation within its\n",
    "electric system and subtracting the total metered net electricity interchange occuring\n",
    "between the BA and its neighboring BAs. Total demand should equal or approximate the sum of demand by subregion.\n",
    "\n",
    " - Net generation: Derived from the metered outpout of electric generating units in a\n",
    "BA's electric system. Generators on the distribution system are typically not included. Total net generation should equal the sum of net generation by energy source.\n",
    "\n",
    " - Total interchange: Net metered tie line flow from one BA to another directly\n",
    "connnected BA. Typically, demand equals net generation minus total interchange.\n",
    "\n",
    "Next it is important to understand the differences between imputed and adjusted data values:\n",
    "\n",
    " - Adjusted values: To incorporate commercial arrangements, such as dynamic scheduling arrangements and interchanges on pseudo ties, BAs normally adjust their metered physical flow values to produce this alternative view of grid operations.\n",
    "  - Imputed values: With imputed values, BAs occasionally report anomalous data values such as blank, zero, negative, and unreasonably high or low values. We perform a basic imputation process for several data elements. The process for imputing is slightly different for Demand, Net Generation, and Total Interchange, but usually, the EIA imputes values if the value is missing or reported as negative, zero, or at least 1.5 times greater than the maximum of past total demand values reported by that BA.\n",
    "\n",
    "Third, there are three sources of bias in this data. These biases include:\n",
    "\n",
    "1. There are several generation BAs that do not directly serve retail customers. Therefore\n",
    "they do not report demand or demand forecasts.\n",
    "2. City of Homestead (HST) has a small number of local generators that do not always produce\n",
    "electricity, so it will not always have net generation to report.\n",
    "3. Dynamic Scheduling and Pseduo ties can introduce some level of bias onto demand, demand\n",
    "forecast, net generation and interchange.\n",
    "\n",
    "Finally, it is important to understand the sources of energy. The sources of energy include                    Coal,\n",
    "         Natural Gas,\n",
    "         Nuclear,\n",
    "         All Petroleum Products,\n",
    "         Hydropower Excluding Pumped Storage,\n",
    "         Pumped Storage',\n",
    "         Solar without Integrated Battery Storage,\n",
    "         Solar with Integrated Battery Storage,\n",
    "         Wind without Integrated Battery Storage,\n",
    "         Wind with Integrated Battery Storage,\n",
    "         Battery Storage,\n",
    "         Other Energy Storage,\n",
    "         Unknown Energy Storage,\n",
    "         Geothermal,\n",
    "         Other Fuel Sources,\n",
    "         Unknown Fuel Sources\n",
    "\n",
    "\n",
    "____________________Double check the adjusted explanation for these columns__________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac673cdc3ddfd70",
   "metadata": {},
   "source": [
    "## Understanding Demand by Balancing Authority\n",
    "\n",
    "We have three demand values that matter to us. We have just demand, adjusted demand, and imputed demand. We can create a new DataFrame comparing the three demands to see how they interact with each other. For most of them, we can see that $Demand_{Adjusted}=Demand+Demand_{Imputed}$. We can do this by Adding up the Demand and Imputed Demand which should equal the Adjusted Demand. We can also add another column that sets this all to zero by taking the adjusted demand and subtracting it from the sum of the demand and imputed demand.\n",
    "\n",
    "### Conclusion\n",
    "From this exploration, we can conclude that using the Adjusted Demand is the most useful metric to use for demand."
   ]
  },
  {
   "cell_type": "code",
   "id": "7db6802b4ee13a63",
   "metadata": {},
   "source": [
    "demand_by_ba = df.groupby([\"Balancing Authority\", \"Region\"])[\"Demand (MW)\"].sum()\n",
    "adjusted_demand_by_ba = df.groupby([\"Balancing Authority\", \"Region\"])[\"Demand (MW) (Adjusted)\"].sum()\n",
    "imputed_demand_by_ba = df.groupby([\"Balancing Authority\", \"Region\"])[\"Demand (MW) (Imputed)\"].sum()\n",
    "\n",
    "demand_df = pd.concat([demand_by_ba, adjusted_demand_by_ba, imputed_demand_by_ba], axis=1)\n",
    "\n",
    "demand_df[\"Demand + Imputed\"] = demand_df[\"Demand (MW)\"] + demand_df[\"Demand (MW) (Imputed)\"]\n",
    "\n",
    "demand_df[\"Check Difference\"] = demand_df[\"Demand (MW) (Adjusted)\"] - (\n",
    "        demand_df[\"Demand (MW)\"] + demand_df[\"Demand (MW) (Imputed)\"])\n",
    "\n",
    "demand_df = demand_df.reset_index()\n",
    "\n",
    "demand_df.to_csv(\"demand_comparison.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a7c4ff8f6c7cac2",
   "metadata": {},
   "source": [
    "## Understanding Generation by Balancing Authority\n",
    "\n",
    "We can run nearly identical code as above to understand generation and look for discrepancies. We can see that, again, for most of the balancing authorities, we have $Generation_{Adjusted}=Generation+Generation_{Imputed}$ once again. Some of them are inaccurate. Next, we'll see which ones are inaccurate in both generation and demand, just in generation, and just in demand.\n",
    "\n",
    "### Conclusion\n",
    "Again, we can see that adjusted generation is the most useful metric to us here."
   ]
  },
  {
   "cell_type": "code",
   "id": "ac48a0d55283ef8f",
   "metadata": {},
   "source": [
    "generation_by_ba = df.groupby([\"Balancing Authority\", \"Region\"])[\"Net Generation (MW)\"].sum()\n",
    "adjusted_generation_by_ba = df.groupby([\"Balancing Authority\", \"Region\"])[\"Net Generation (MW) (Adjusted)\"].sum()\n",
    "imputed_generation_by_ba = df.groupby([\"Balancing Authority\", \"Region\"])[\"Net Generation (MW) (Imputed)\"].sum()\n",
    "\n",
    "generation_df = pd.concat([generation_by_ba, adjusted_generation_by_ba, imputed_generation_by_ba], axis=1)\n",
    "\n",
    "generation_df[\"Generation + Imputed\"] = generation_df[\"Net Generation (MW)\"] + generation_df[\n",
    "    \"Net Generation (MW) (Imputed)\"]\n",
    "\n",
    "generation_df[\"Check Difference\"] = generation_df[\"Net Generation (MW) (Adjusted)\"] - (\n",
    "        generation_df[\"Net Generation (MW)\"] + generation_df[\"Net Generation (MW) (Imputed)\"])\n",
    "\n",
    "generation_df = generation_df.reset_index()\n",
    "\n",
    "generation_df.to_csv(\"generation_comparison.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9616d5372e7a68a2",
   "metadata": {},
   "source": [
    "## Understanding Inaccuracies\n",
    "\n",
    "We will now locate balancing authorities where the check difference is non zero. We can then turn it into a set and find the intersections and differences. This will tell us which balancing authorities have inaccuracies and whether it is just in demand, generation, or both.\n",
    "\n",
    "### Conclusion\n",
    "One interesting thing we can see from this, is that in a lot of the inaccurate imputations, the region is NW (Northwest). One hypothesis is that balancing authorities in the Northwest may use different types of power generation than other regions. We can now look into seeing which type of power generation is used most per region."
   ]
  },
  {
   "cell_type": "code",
   "id": "716da638f2a63468",
   "metadata": {},
   "source": [
    "def show_with_region(df, ba_list):\n",
    "    return df[df[\"Balancing Authority\"].isin(ba_list)][[\"Balancing Authority\", \"Region\"]]\n",
    "\n",
    "\n",
    "demand_discrep = demand_df.loc[demand_df[\"Check Difference\"] != 0, \"Balancing Authority\"]\n",
    "gen_discrep = generation_df.loc[generation_df[\"Check Difference\"] != 0, \"Balancing Authority\"]\n",
    "\n",
    "demand_discrep = set(demand_discrep)\n",
    "gen_discrep = set(gen_discrep)\n",
    "\n",
    "both = demand_discrep.intersection(gen_discrep)\n",
    "only_demand = demand_discrep.difference(gen_discrep)\n",
    "only_gen = gen_discrep.difference(demand_discrep)\n",
    "\n",
    "print(\"Discrepancies in both demand and generation:\")\n",
    "print(show_with_region(demand_df, both))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Discrepancies only in demand:\")\n",
    "print(show_with_region(demand_df, only_demand))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Discrepancies only in generation:\")\n",
    "print(show_with_region(generation_df, only_gen))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d03eb71934652274",
   "metadata": {},
   "source": [
    "## Understanding Sources of Power Generation\n",
    "\n",
    "This graph will plot out how much power is generated in each region and what type of power is used in said region. This graph definitely looks overwhelming, but not to worry. You can focus on the orange section on the Northwest region. Notice that the amount of hydropower for every other region is significantly lower."
   ]
  },
  {
   "cell_type": "code",
   "id": "605ffc5689aa4303",
   "metadata": {},
   "source": [
    "adjusted_cols = [col for col in df.columns if \"(Adjusted)\" in col and \"Net Generation (MW) from\" in col]\n",
    "df[adjusted_cols] = df[adjusted_cols].fillna(0)\n",
    "gen_by_region = df.groupby(\"Region\")[adjusted_cols].sum()\n",
    "custom_colors = [\n",
    "    \"#1f77b4\",\n",
    "    \"#9467bd\",\n",
    "    \"#2ca02c\",\n",
    "    \"#d62728\",\n",
    "    \"#ff7f0e\",\n",
    "    \"#8c564b\",\n",
    "    \"#e377c2\",\n",
    "    \"#7f7f7f\",\n",
    "    \"#bcbd22\",\n",
    "    \"#17becf\",\n",
    "    \"#aec7e8\",\n",
    "    \"#ffbb78\",\n",
    "    \"#98df8a\",\n",
    "    \"#ff9896\",\n",
    "    \"#c5b0d5\",\n",
    "    \"#c49c94\",\n",
    "    \"#f7b6d2\",\n",
    "    \"#c7c7c7\",\n",
    "    \"#dbdb8d\",\n",
    "    \"#9edae5\",\n",
    "]\n",
    "\n",
    "ax = gen_by_region.plot(\n",
    "    kind=\"bar\",\n",
    "    stacked=True,\n",
    "    figsize=(14, 7),\n",
    "    color=custom_colors\n",
    ")\n",
    "\n",
    "plt.title(\"Adjusted Net Generation by Fuel Source per Region\")\n",
    "plt.ylabel(\"Net Generation (MW)\")\n",
    "plt.xlabel(\"Region\")\n",
    "plt.legend(\n",
    "    title=\"Fuel Source\",\n",
    "    bbox_to_anchor=(1.05, 1),\n",
    "    loc=\"upper left\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Demand vs. Generation\n",
    "\n",
    "We'll now take a look at demand for March 2025. Why March 2025? Because my birthday is in March. Allegedly. One of the interesting things we're doing here is scaling it to the 99th percentile. If you don't, then you see a pretty egregious outlier. There's even one outlier that has -50,000MW of generation. This does help us show that it does appear to be a pretty linear relationship. As demand goes up, so does generation. While this may seem fairly obvious, any point of interest where demand is too high would be interesting. Showing us the 99th percentile shows us that there are actually not that many outliers."
   ],
   "id": "5ad1207ebd4237cb"
  },
  {
   "cell_type": "code",
   "id": "532f33b4",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7), sharex=False, sharey=False)\n",
    "\n",
    "# --- Left: full scale ---\n",
    "axes[0].scatter(df[\"Demand (MW)\"], df[\"Net Generation (MW)\"], alpha=0.5)\n",
    "max_val = max(df[\"Demand (MW)\"].max(), df[\"Net Generation (MW)\"].max())\n",
    "axes[0].plot([0, max_val], [0, max_val], 'r--', label=\"1:1 Line\")\n",
    "axes[0].set_title(\"Full Scale\")\n",
    "axes[0].set_xlabel(\"Actual Demand (MW)\")\n",
    "axes[0].set_ylabel(\"Net Generation (MW)\")\n",
    "axes[0].legend()\n",
    "\n",
    "# --- Right: zoomed to 99th percentile ---\n",
    "axes[1].scatter(df[\"Demand (MW)\"], df[\"Net Generation (MW)\"], alpha=0.5)\n",
    "axes[1].plot([0, max_val], [0, max_val], 'r--', label=\"1:1 Line\")\n",
    "xlim = df[\"Demand (MW)\"].quantile(0.99)\n",
    "ylim = df[\"Net Generation (MW)\"].quantile(0.99)\n",
    "axes[1].set_xlim(0, xlim)\n",
    "axes[1].set_ylim(0, ylim)\n",
    "axes[1].set_title(\"Zoomed to 99th Percentile\")\n",
    "axes[1].set_xlabel(\"Actual Demand (MW)\")\n",
    "axes[1].set_ylabel(\"Net Generation (MW)\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle(\"Demand vs Net Generation — March 2025, NW Region\", y=1.02)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f5193b9",
   "metadata": {},
   "source": [
    "# Filter NW + March 2025\n",
    "#nw = df[\n",
    "#    (df[\"Region\"] == \"NW\") &\n",
    "#    (df[\"UTC Time at End of Hour\"].dt.year == 2025) &\n",
    "#    (df[\"UTC Time at End of Hour\"].dt.month == 3)\n",
    "#]\n",
    "\n",
    "# Aggregate to daily totals\n",
    "daily = df.groupby(df[\"UTC Time at End of Hour\"].dt.date)[[\"Demand (MW)\", \"Net Generation (MW)\"]].sum()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(daily.index, daily[\"Demand (MW)\"], color=\"blue\", label=\"Actual Demand (MW)\")\n",
    "plt.plot(daily.index, daily[\"Net Generation (MW)\"], color=\"red\", label=\"Net Generation (MW)\")\n",
    "\n",
    "plt.title(\"NW Region — Daily Demand vs Net Generation (2025)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Total MW (Daily Sum)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Plotting Time Series Decomposition\n",
    "\n",
    "Since this is time series data, we can plot out the residuals to see the trend, seasonality, and residuals. We can see that the trend varies depending on the region. This makes sense. Some regions have a very harsh summer and demand more air conditioning. Some regions have colder winters and demand more heat, but may have nicer summers and not need as much air conditioning. We can also see that there are 12 \"peaks\" in our seasonality. This aligns with our dataset being 12 months. We can assume there likely is a peak day of the month. However, it's unclear if each peak day of the month has any usefulness to us or whether it's random."
   ],
   "id": "51bcf759b177e42d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "regions = df[\"Region\"].unique()\n",
    "\n",
    "for region in regions:\n",
    "    adjusted_mw_perday_oneregion = df[df[\"Region\"] == region].set_index(\"UTC Time at End of Hour\")[\n",
    "        \"Net Generation (MW) (Adjusted)\"].resample(\"D\").sum()\n",
    "\n",
    "    result = seasonal_decompose(adjusted_mw_perday_oneregion, model=\"additive\", period=30)\n",
    "\n",
    "    result.plot()\n",
    "    plt.suptitle(f\"Seasonal Decomposition of Adjusted Net Generation ({region})\", y=1.02)\n",
    "    plt.show()"
   ],
   "id": "f72f23de80af96d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Finding the spikes\n",
    "\n",
    "We can see that after plotting our data, there is actually no pattern to the spikes that we could find. Initially, we thought that maybe it was because there was an extra surge on the fist or last day of the month. Since this is grouped by region and not balancing authority, it's hard to pinpoint it to one specific balancing authority and whether they potentially have an artificial influx due to data or business processing."
   ],
   "id": "6d4124f1a14f3aec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "adjusted_mw_perday_allregions = df.set_index(\"UTC Time at End of Hour\").groupby(\"Region\")[\n",
    "    \"Net Generation (MW) (Adjusted)\"].resample(\n",
    "    \"D\").sum().reset_index()\n",
    "\n",
    "# Add a year month column, we need just the year and month to properly group by and find the max\n",
    "adjusted_mw_perday_allregions[\"YearMonth\"] = adjusted_mw_perday_allregions[\"UTC Time at End of Hour\"].dt.to_period(\"M\")\n",
    "\n",
    "# Now we can find the highest day per month and find adjusted net generation\n",
    "max_days = adjusted_mw_perday_allregions.loc[adjusted_mw_perday_allregions.groupby([\"Region\", \"YearMonth\"])[\n",
    "    \"Net Generation (MW) (Adjusted)\"].idxmax()].sort_values([\"Region\", \"UTC Time at End of Hour\"])\n",
    "\n",
    "regions = adjusted_mw_perday_allregions[\"Region\"].unique()\n",
    "\n",
    "for region in regions:\n",
    "    subset = adjusted_mw_perday_allregions[adjusted_mw_perday_allregions[\"Region\"] == region]\n",
    "    peaks = max_days[max_days[\"Region\"] == region]\n",
    "\n",
    "    # Plot the actual generation in a line\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(\n",
    "        subset[\"UTC Time at End of Hour\"],\n",
    "        subset[\"Net Generation (MW) (Adjusted)\"],\n",
    "        label=\"Daily Total\"\n",
    "    )\n",
    "\n",
    "    # Scatter plot the peak day (it's just one dot per month)\n",
    "    plt.scatter(\n",
    "        peaks[\"UTC Time at End of Hour\"],\n",
    "        peaks[\"Net Generation (MW) (Adjusted)\"],\n",
    "        color=\"red\", s=80, label=\"Monthly Peak\"\n",
    "    )\n",
    "\n",
    "    # We want to see the specific day of the month for each month\n",
    "    for _, row in peaks.iterrows():\n",
    "        day_label = row[\"UTC Time at End of Hour\"].day\n",
    "        month_label = row[\"UTC Time at End of Hour\"].month\n",
    "        label = f\"{month_label}/{day_label}\"\n",
    "        plt.text(\n",
    "            row[\"UTC Time at End of Hour\"],\n",
    "            row[\"Net Generation (MW) (Adjusted)\"],\n",
    "            str(label),\n",
    "            color=\"black\", fontsize=9, ha=\"center\", va=\"bottom\"\n",
    "        )\n",
    "\n",
    "    plt.title(f\"Daily Adjusted Net Generation with Monthly Peaks ({region})\")\n",
    "    plt.ylabel(\"Net Generation (MW)\")\n",
    "    plt.xlabel(\"Date (UTC)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "12eb9e3692ebef2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Transitioning to AI Datacenters\n",
    "\n",
    "Now that we have a good idea of what our power grid looks like, we can focus on the more interesting part of our project: datacenters. Why do the exercises above? Exploring the data is quite important and familiarity helps when you want to dive deeper into it. Understanding how the power grid works in general helps us drill down on datacenters more effectively. The most important thing we learned is that supply and demand must be kept very closely together. Any shortage is bad. We also know that the power grid is interconnected. Balancing authorities can transfer power from themselves to other balancing authorities in need. Translating this information to datacenters will not be an easy task.\n",
    "\n",
    "Firstly, we have a list of datacenters by state. However, each BA does not \"belong\" to a state. The good news is that we're not interested in all 50 states for the scope of this project. We're mostly interested in Virginia, California, and Texas. They by far have the most amount of datacenters. What we can do is figure out the interchange between BA's. If a BA from another state is exporting a lot to the BA that handles Virginia, we can assume it is under strain. Doing it by region alone is not enough. This can be proven quite easily. Take a look at the datacenters in Virginia and West Virginia. WV has three whereas VA has the most. They technically are under the same BA. It's not enough to just look at the region nor state. BA is the only way we can get granular enough.\n",
    "\n",
    "Also, we must understand that generation and demand itself does not tell the full story. It is important that both must be kept in sync. Now, we're taking a deeper look into *how* this happens between balancing authorities."
   ],
   "id": "1ac63da162794471"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "eia_api_key = os.getenv(\"EIA_API_KEY\")\n",
    "\n",
    "url = \"https://api.eia.gov/v2/electricity/rto/interchange-data/data/\"\n",
    "\n",
    "\n",
    "def fetch_interchange_to_csv(start=\"2025-01-01T00\", end=\"2025-07-31T00\", output_file=\"interchange_data.csv\",\n",
    "                             max_rows=None):\n",
    "    offset = 0\n",
    "    length = 5000\n",
    "    total_fetched = 0\n",
    "    header_written = False\n",
    "\n",
    "    # We can't use Pandas. It will run out of memory\n",
    "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        print(\"Now calling the API. This will take a few minutes, please be patient.\")\n",
    "        while True:\n",
    "            params = {\n",
    "                \"frequency\": \"hourly\",\n",
    "                \"data[0]\": \"value\",\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"sort[0][column]\": \"period\",\n",
    "                \"sort[0][direction]\": \"desc\",\n",
    "                \"offset\": offset,\n",
    "                \"length\": length,\n",
    "                \"api_key\": eia_api_key,\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "            data = response.json()\n",
    "\n",
    "            if \"response\" not in data:\n",
    "                print(\"Bad response\", data)\n",
    "                break\n",
    "\n",
    "            rows = data[\"response\"][\"data\"]\n",
    "            if not rows:\n",
    "                print(\"Data successfully fetched!\")\n",
    "                break\n",
    "\n",
    "            interchange_ba = pd.json_normalize(rows)\n",
    "            interchange_ba.rename(\n",
    "                columns={\n",
    "                    \"fromba\": \"FromBA\",\n",
    "                    \"fromba-name\": \"FromBAName\",\n",
    "                    \"toba\": \"ToBA\",\n",
    "                    \"toba-name\": \"ToBAName\",\n",
    "                    \"value\": \"MWh\",\n",
    "                },\n",
    "                inplace=True,\n",
    "            )\n",
    "\n",
    "            if \"period\" in interchange_ba.columns:\n",
    "                interchange_ba[\"period\"] = pd.to_datetime(interchange_ba[\"period\"], errors=\"coerce\")\n",
    "            if \"MWh\" in interchange_ba.columns:\n",
    "                interchange_ba[\"MWh\"] = pd.to_numeric(interchange_ba[\"MWh\"], errors=\"coerce\")\n",
    "\n",
    "            interchange_ba.to_csv(f, index=False, header=not header_written)\n",
    "            header_written = True\n",
    "\n",
    "            total_fetched += len(interchange_ba)\n",
    "            offset += length\n",
    "\n",
    "            if max_rows and total_fetched >= max_rows:\n",
    "                break\n",
    "\n",
    "    print(f\"Saved {total_fetched} rows to {output_file}\")\n",
    "\n",
    "\n",
    "fetch_interchange_to_csv(start=\"2025-08-01T00\", end=\"2025-08-31T00\", output_file=\"interchange_aug_2025.csv\",\n",
    "                         max_rows=None)"
   ],
   "id": "b7f959a1e6808ef3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Now that we have the CSV of the BA transactions, we can add the region per each for better visibility. We also see that some of them are negative megawatts. This discrepancy is documented by the EIA themselves. As per the EIA:\n",
    "> **External interchange reporting consistency**: BAs are physically connected to their neighboring BAs by one or more transmission lines. These lines have meters that measure the flow of electricity between BAs. Interchange reported by BA 1 with BA 2 should be the exact reverse of the interchange reported by BA 2 with BA 1. For example, if BA 1 reports +100 MW interchange with BA 2, BA 2 should report -100 MW with BA 1.\n",
    " Correcting incorrect interchange values for one or both of the BAs in a BA pair is particularly challenging because it is difficult to identify misreported data and the reason for mismatches. We have found that a number of BAs are not reporting using a data source with checked-out values. In addition, some BAs are not appropriately accounting for pseudo-ties or dynamic schedules in their reporting.\n",
    "\n",
    "In short, a positive or negative MWh actually *does* carry meaning. It tells us direction and if it's flipped or not. If the MWh is negative, it means the From and To are flipped. We added a new column to show that and make it crystal clear. However, we have not flipped any of the negatives to positives or vice versa.\n",
    "\n",
    "We did feature engineer the region. However, sometimes the BA is just \"CAL\", for example. This is a region in itself, not a BA. We want to drop these since it's just a pseudo grouping. This dataset combines the generic \"CAL\" regions *and* the BA data. By dropping BAs that are equal to a region, we get solely BA to BA data."
   ],
   "id": "e31aff7a49b6f1b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ba_lookup_path = \"EIA930_Reference_Tables.xlsx\"\n",
    "df_lookup = pd.read_excel(ba_lookup_path, engine=\"openpyxl\")\n",
    "df_lookup = df_lookup[[\"BA Code\", \"Region/Country Code\", \"Region/Country Name\"]].rename(\n",
    "    columns={\n",
    "        \"BA Code\": \"BA\",\n",
    "        \"Region/Country Code\": \"RegionCode\",\n",
    "        \"Region/Country Name\": \"RegionName\"\n",
    "    }\n",
    ")\n",
    "\n",
    "interchange_path = \"interchange_aug_2025.csv\"\n",
    "df_interchange = pd.read_csv(interchange_path, parse_dates=[\"period\"])\n",
    "\n",
    "df_interchange = df_interchange.merge(df_lookup, left_on=\"FromBA\", right_on=\"BA\", how=\"left\").rename(\n",
    "    columns={\"RegionCode\": \"FromRegionCode\", \"RegionName\": \"FromRegionName\"}).drop(columns=[\"BA\"])\n",
    "\n",
    "df_interchange = df_interchange.merge(df_lookup, left_on=\"ToBA\", right_on=\"BA\", how=\"left\").rename(\n",
    "    columns={\"RegionCode\": \"ToRegionCode\", \"RegionName\": \"ToRegionName\"}).drop(columns=[\"BA\"])\n",
    "\n",
    "pseudo_bas = [\n",
    "    \"CAL\", \"MIDA\", \"MIDW\", \"CENT\", \"NE\", \"NW\", \"NY\", \"SE\", \"SW\",\n",
    "    \"TEN\", \"TEX\", \"US48\", \"CAN\", \"MEX\"\n",
    "]\n",
    "\n",
    "df_interchange = df_interchange[\n",
    "    ~df_interchange[\"FromBA\"].isin(pseudo_bas) &\n",
    "    ~df_interchange[\"ToBA\"].isin(pseudo_bas)\n",
    "    ].copy()\n",
    "\n",
    "# This gets us the \"true\" direction\n",
    "df_interchange[\"Direction\"] = df_interchange[\"MWh\"].apply(lambda x: \"From -> To\" if x > 0 else \"To -> From\")\n",
    "\n",
    "df_interchange.to_csv(\"interchange_aug_2025_with_regions.csv\", index=False)\n",
    "print(\"Done with feature engineering. Added region and imputed regions to new file.\")\n"
   ],
   "id": "52233b143e2805f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_normalized = df_interchange.copy()\n",
    "\n",
    "mask_negative = df_normalized[\"MWh\"] < 0\n",
    "df_normalized.loc[mask_negative, [\"FromBA\", \"ToBA\"]] = df_normalized.loc[mask_negative, [\"ToBA\", \"FromBA\"]].values\n",
    "df_normalized.loc[mask_negative, [\"FromBAName\", \"ToBAName\"]] = df_normalized.loc[\n",
    "    mask_negative, [\"ToBAName\", \"FromBAName\"]].values\n",
    "df_normalized.loc[mask_negative, [\"FromRegionCode\", \"ToRegionCode\"]] = df_normalized.loc[\n",
    "    mask_negative, [\"ToRegionCode\", \"FromRegionCode\"]].values\n",
    "df_normalized.loc[mask_negative, [\"FromRegionName\", \"ToRegionName\"]] = df_normalized.loc[\n",
    "    mask_negative, [\"ToRegionName\", \"FromRegionName\"]].values\n",
    "df_normalized.loc[mask_negative, \"MWh\"] = -df_normalized.loc[mask_negative, \"MWh\"]\n",
    "\n",
    "exports_by_ba = df_normalized.groupby([\"FromBA\", \"FromBAName\", \"FromRegionCode\", \"FromRegionName\"])[\n",
    "    \"MWh\"].sum().reset_index().rename(columns={\"MWh\": \"TotalExportsMWh\"}).sort_values(\"TotalExportsMWh\",\n",
    "                                                                                      ascending=False)\n",
    "\n",
    "imports_by_ba = df_normalized.groupby([\"ToBA\", \"ToBAName\", \"ToRegionCode\", \"ToRegionName\"])[\n",
    "    \"MWh\"].sum().reset_index().rename(columns={\"MWh\": \"TotalImportsMWh\"}).sort_values(\"TotalImportsMWh\",\n",
    "                                                                                      ascending=False)\n",
    "\n",
    "top_flows = df_normalized.groupby([\"FromBA\", \"FromBAName\", \"ToBA\", \"ToBAName\"])[\"MWh\"].sum().reset_index().sort_values(\"MWh\", ascending=False)\n",
    "\n",
    "#exports_by_ba.to_csv(\"ba_exports_summary.csv\", index=False)\n",
    "#imports_by_ba.to_csv(\"ba_imports_summary.csv\", index=False)\n",
    "top_flows.to_csv(\"ba_top_flows.csv\", index=False)"
   ],
   "id": "934a0fd148a2a265",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Top 10 BA to BA Flows:\")\n",
    "top_flows.head(10)"
   ],
   "id": "bc657389bfc7b375",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d47662730cf8c5a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### AI Disclosure\n",
    "\n",
    "ChatGPT was used to help generate the Matplotlib graph showcasing the \"Adjusted Net Generation by Fuel Source per Region\". Since there were a lot of columns, ChatGPT was used to assist in attempting to visualize the large amount of columns in a way that made sense.\n",
    "\n",
    "ChatGPT was also used to assist in revising an initial attempt to download data from the EIA API. I ran into issues running out of memory and used ChatGPT to help rethink how such large amounts of data are downloaded and processed to disk rather than to memory.\n",
    "\n",
    "We also used ChatGPT to help with some minor clarifications with syntax relating to Pandas."
   ],
   "id": "53ac2571e84c8758"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
